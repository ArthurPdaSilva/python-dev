{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e7123",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"files/apostila.pdf\",\n",
    "    \"files/LLM.pdf\",\n",
    "    ]\n",
    "\n",
    "pages = []\n",
    "for path in paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages.extend(loader.load())\n",
    "\n",
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = recur_split.split_documents(pages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata['source'] = doc.metadata['source'].replace('arquivos/', '')\n",
    "    doc.metadata['doc_id'] = i\n",
    "\n",
    "\n",
    "path = 'arquivos/chat_retrieval_db'\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0196768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(search_type='mmr'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"O que é Hugging Face e como faço para acessá-lo?\"\n",
    "result = chat_chain.run(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689245b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for RefineDocumentsChain\nprompt\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[32m      3\u001b[39m chain_prompt = PromptTemplate.from_template(\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"Utilize o contexto fornecido para responder a pergunta ao final. \u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03mSe você não sabe a resposta, apenas diga que não sabe e não invente uma resposta.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     15\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m chat_chain = \u001b[43mRetrievalQA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvectordb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmmr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# mmr = MMR também procura relevância, mas evita repetição.\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchain_type_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrefine\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 'refine' permite que o modelo refine a resposta com base em múltiplos documentos.\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_source_documents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Retorna os documentos de origem que foram usados para gerar a resposta.\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mO que é Hugging Face e como faço para acessá-lo?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m result = chat_chain.invoke({\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: question})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\OneDrive\\Documentos\\GitHub\\python-dev\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:105\u001b[39m, in \u001b[36mBaseRetrievalQA.from_chain_type\u001b[39m\u001b[34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[32m    104\u001b[39m _chain_type_kwargs = chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m combine_documents_chain = \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_chain_type_kwargs\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain=combine_documents_chain, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\OneDrive\\Documentos\\GitHub\\python-dev\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\question_answering\\__init__.py:249\u001b[39m, in \u001b[36mload_qa_chain\u001b[39m\u001b[34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    246\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    247\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping.keys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\OneDrive\\Documentos\\GitHub\\python-dev\\langchain\\.venv\\Lib\\site-packages\\langchain\\chains\\question_answering\\__init__.py:206\u001b[39m, in \u001b[36m_load_refine_chain\u001b[39m\u001b[34m(llm, question_prompt, refine_prompt, document_variable_name, initial_response_name, refine_llm, verbose, callback_manager, callbacks, **kwargs)\u001b[39m\n\u001b[32m    198\u001b[39m _refine_llm = refine_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[32m    199\u001b[39m refine_chain = LLMChain(\n\u001b[32m    200\u001b[39m     llm=_refine_llm,\n\u001b[32m    201\u001b[39m     prompt=_refine_prompt,\n\u001b[32m   (...)\u001b[39m\u001b[32m    204\u001b[39m     callbacks=callbacks,\n\u001b[32m    205\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRefineDocumentsChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_llm_chain\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefine_llm_chain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefine_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_response_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_response_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\OneDrive\\Documentos\\GitHub\\python-dev\\langchain\\.venv\\Lib\\site-packages\\pydantic\\v1\\main.py:347\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(__pydantic_self__, **data)\u001b[39m\n\u001b[32m    345\u001b[39m values, fields_set, validation_error = validate_model(__pydantic_self__.\u001b[34m__class__\u001b[39m, data)\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    349\u001b[39m     object_setattr(__pydantic_self__, \u001b[33m'\u001b[39m\u001b[33m__dict__\u001b[39m\u001b[33m'\u001b[39m, values)\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for RefineDocumentsChain\nprompt\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chain_prompt = PromptTemplate.from_template(\n",
    "\"\"\"Utilize o contexto fornecido para responder a pergunta ao final. \n",
    "Se você não sabe a resposta, apenas diga que não sabe e não invente uma resposta.\n",
    "Utilize três frases no máximo, mantenha a resposta concisa.\n",
    "Fale em gírias nordestinas e de gangster, pois você é um gangster nordestino estiloso.\n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Pergunta: {question}\n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(search_type='mmr'), # mmr = MMR também procura relevância, mas evita repetição.\n",
    "    # chain_type_kwargs={\"prompt\": chain_prompt},\n",
    "    chain_type='refine', # 'refine' permite que o modelo refine a resposta com base em múltiplos documentos. Mas o chain_type_kwargs tem que ser removido.\n",
    "    return_source_documents=True # Retorna os documentos de origem que foram usados para gerar a resposta.\n",
    ")\n",
    "\n",
    "question = \"O que é Hugging Face e como faço para acessá-lo?\"\n",
    "result = chat_chain.invoke({\"query\": question})\n",
    "print(result[\"result\"])\n",
    "print(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434feaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING\n",
    "# Para ver os logs de depuração, você pode usar o seguinte código:\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "pergunta = 'O que é Hugging Face e como faço para acessá-lo?'\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "\n",
    "set_debug(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
