{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e7123",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"files/apostila.pdf\",\n",
    "    \"files/LLM.pdf\",\n",
    "    ]\n",
    "\n",
    "pages = []\n",
    "for path in paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages.extend(loader.load())\n",
    "\n",
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = recur_split.split_documents(pages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata['source'] = doc.metadata['source'].replace('arquivos/', '')\n",
    "    doc.metadata['doc_id'] = i\n",
    "\n",
    "\n",
    "path = 'arquivos/chat_retrieval_db'\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0196768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(search_type='mmr'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"O que é Hugging Face e como faço para acessá-lo?\"\n",
    "result = chat_chain.run(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "689245b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face é uma empresa e uma comunidade que desenvolve e mantém uma vasta gama de modelos de aprendizado de máquina, especialmente voltados para o processamento de linguagem natural (NLP). A biblioteca \"Transformers\" da Hugging Face permite que desenvolvedores e pesquisadores utilizem modelos de linguagem avançados de forma acessível e eficiente.\n",
      "\n",
      "Um dos maiores avanços nos modelos de linguagem, como os oferecidos pelo Hugging Face, foi a integração do feedback humano diretamente no processo de treinamento. Isso melhorou significativamente a capacidade dos modelos de entender e gerar texto de maneira mais natural e relevante. Além disso, o lançamento de interfaces como o ChatGPT democratizou o acesso a esses modelos, permitindo que qualquer pessoa com acesso à internet interaja com um dos LLMs (Modelos de Linguagem de Grande Escala) mais avançados.\n",
      "\n",
      "Para acessar o Hugging Face, você pode seguir estas etapas:\n",
      "\n",
      "1. **Visitar o site**: Vá para o site oficial da Hugging Face em [huggingface.co](https://huggingface.co).\n",
      "\n",
      "2. **Criar uma conta**: Se você deseja usar alguns recursos, como hospedar modelos ou acessar dados, pode precisar criar uma conta gratuita.\n",
      "\n",
      "3. **Explorar a biblioteca**: A biblioteca Transformers pode ser instalada usando o comando `pip install transformers` no terminal.\n",
      "\n",
      "4. **Consultar a documentação**: A Hugging Face oferece uma documentação abrangente que pode ajudar você a começar a usar os modelos, com tutoriais e exemplos práticos.\n",
      "\n",
      "5. **Experimentar com modelos**: Após a instalação, você pode carregar e usar modelos pré-treinados diretamente em seu código Python. Abaixo, aqui estão alguns exemplos simples de como usar a biblioteca Transformers em Python:\n",
      "\n",
      "```python\n",
      "from transformers import pipeline\n",
      "\n",
      "# Crie um pipeline de geração de texto\n",
      "generator = pipeline('text-generation', model='gpt-2')\n",
      "\n",
      "# Gere texto\n",
      "result = generator(\"A inteligência artificial pode\", max_length=50, num_return_sequences=1)\n",
      "\n",
      "print(result)\n",
      "```\n",
      "\n",
      "Se você estiver usando o Databricks, pode facilmente integrar a biblioteca Transformers em seus notebooks e aproveitar a infraestrutura de computação para trabalhar com LLMs de forma eficiente.\n",
      "\n",
      "Essa combinação de maior acessibilidade, avanços em feedback humano e aumento da potência computacional tornou os modelos da Hugging Face uma escolha poderosa e flexível para aplicações de linguagem natural, sem as limitações que frequentemente acompanham serviços proprietários.\n",
      "[Document(page_content='Atualmente, requer um pouco mais de esforço para pegar um modelo de código aberto e começar a usá-lo, mas o progresso está ocorrendo muito rapidamente para torná-los mais acessíveis aos usuários. Na Databricks, por exemplo, fizemos melhorias em frameworks de código aberto como o MLflow para tornar muito fácil para alguém com um pouco de experiência em Python pegar qualquer modelo transformador da Hugging Face e usá-lo como um objeto Python. Muitas vezes, você pode encontrar um modelo de código aberto que resolve seu problema específico e que é várias ordens de grandeza menor que o ChatGPT, permitindo que você traga o modelo para seu ambiente e hospede-o você mesmo. Isso significa que você pode manter os dados sob seu controle para preocupações com privacidade e governança, além de gerenciar seus custos. Outra grande vantagem de usar modelos de código aberto é a capacidade de ajustá-los aos seus próprios dados', metadata={'doc_id': 75, 'page': 6, 'source': 'files/LLM.pdf'}), Document(page_content='. Além disso, devido aos recursos computacionais necessários, esses serviços não são gratuitos além de um uso muito limitado, então o custo se torna um fator ao aplicá-los em grande escala. Resumindo: serviços proprietários são ótimos para usar se você tiver tarefas muito complexas, tiver disposição para compartilhar seus dados com terceiros e quiser incorrer em custos ao operar em escala significativa.   Modelos de código aberto A outra opção para modelos de linguagem é recorrer à comunidade de código aberto, onde houve um crescimento igualmente explosivo nos últimos anos. Comunidades como a Hugging Face reúnem centenas de milhares de modelos de contribuidores que podem ajudar a resolver muitos casos de uso específicos, como geração de texto, resumo e classificação. A comunidade de código aberto está rapidamente alcançando o desempenho dos modelos proprietários, mas ainda não conseguiu igualar o desempenho de algo como o GPT-4.', metadata={'doc_id': 73, 'page': 5, 'source': 'files/LLM.pdf'}), Document(page_content='. Notavelmente, um dos maiores saltos de desempenho veio da integração do feedback humano diretamente no processo de treinamento. MAIOR ACESSIBILIDADE  O lançamento do ChatGPT abriu as portas para qualquer pessoa com acesso à internet interagir com um dos LLMs mais avançados por meio de uma interface web simples. Isso trouxe os impressionantes avanços dos LLMs para o centro das atenções, uma vez que anteriormente esses modelos mais poderosos estavam disponíveis apenas para pesquisadores com recursos significativos e conhecimento técnico profundo.  AUMENTO DA POTÊNCIA COMPUTACIONAL  A disponibilidade de recursos de computação mais poderosos, como unidades de processamento gráfico (GPUs), e melhores técnicas de processamento de dados permitiu que os pesquisadores treinassem modelos muito maiores, melhorando o desempenho desses modelos de linguagem', metadata={'doc_id': 64, 'page': 3, 'source': 'files/LLM.pdf'}), Document(page_content='. Se suas mãos já estão tremendo de emoção e você já tem algum conhecimento prático de Python e Databricks, forneceremos alguns ótimos exemplos com código de exemplo que podem ajudar você a começar a trabalhar com LLMs imediatamente.', metadata={'doc_id': 79, 'page': 7, 'source': 'files/LLM.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chain_prompt = PromptTemplate.from_template(\n",
    "\"\"\"Utilize o contexto fornecido para responder a pergunta ao final. \n",
    "Se você não sabe a resposta, apenas diga que não sabe e não invente uma resposta.\n",
    "Utilize três frases no máximo, mantenha a resposta concisa.\n",
    "Fale em gírias nordestinas e de gangster, pois você é um gangster nordestino estiloso.\n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Pergunta: {question}\n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(search_type='mmr'), # mmr = MMR também procura relevância, mas evita repetição.\n",
    "    # chain_type_kwargs={\"prompt\": chain_prompt},\n",
    "    chain_type='refine', # 'refine' permite que o modelo refine a resposta com base em múltiplos documentos. Mas o chain_type_kwargs tem que ser removido.\n",
    "    return_source_documents=True # Retorna os documentos de origem que foram usados para gerar a resposta.\n",
    ")\n",
    "\n",
    "question = \"O que é Hugging Face e como faço para acessá-lo?\"\n",
    "result = chat_chain.invoke({\"query\": question})\n",
    "print(result[\"result\"])\n",
    "print(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434feaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING\n",
    "# Para ver os logs de depuração, você pode usar o seguinte código:\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "pergunta = 'O que é Hugging Face e como faço para acessá-lo?'\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "\n",
    "set_debug(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
